{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jJdDiQiRXFY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "!pip install mtcnn\n",
        "!pip install opencv-python\n",
        "\n",
        "# This will list the files in 'My Drive', including the shortcuts.\n",
        "drive_root = '/content/drive/MyDrive/'\n",
        "os.listdir(drive_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VOZrfRiRloT"
      },
      "source": [
        "to get frames for original & manipulated and perfrom mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wds63R5mRgkZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "from mtcnn import MTCNN\n",
        "\n",
        "detector = MTCNN()\n",
        "log_file = \"processed_frames.log\"\n",
        "\n",
        "# Load already processed paths\n",
        "processed = set()\n",
        "if os.path.exists(log_file):\n",
        "    with open(log_file, 'r') as f:\n",
        "        processed = set(line.strip() for line in f.readlines())\n",
        "\n",
        "def process_frame(frame_path):\n",
        "    if frame_path in processed:\n",
        "        return False  # Not newly processed\n",
        "\n",
        "    try:\n",
        "        image = cv2.imread(frame_path)\n",
        "        if image is None:\n",
        "            return False\n",
        "\n",
        "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        faces = detector.detect_faces(rgb_image)\n",
        "\n",
        "        for face in faces:\n",
        "            confidence = face['confidence']\n",
        "            box = face['box']\n",
        "            keypoints = face['keypoints']\n",
        "\n",
        "            if confidence < 0.90 or box[2] < 50 or box[3] < 50:\n",
        "                continue\n",
        "            if not all(k in keypoints for k in ['left_eye', 'right_eye', 'nose', 'mouth_left', 'mouth_right']):\n",
        "                continue\n",
        "            if abs(keypoints['left_eye'][0] - keypoints['right_eye'][0]) < 15:\n",
        "                continue\n",
        "\n",
        "            x, y, w, h = box\n",
        "            x, y = max(0, x), max(0, y)\n",
        "            cropped_face = image[y:y+h, x:x+w]\n",
        "\n",
        "            cv2.rectangle(cropped_face, (0, 0), (w, h), (0, 255, 0), 2)\n",
        "            cv2.imwrite(frame_path, cropped_face)\n",
        "\n",
        "            with open(log_file, 'a') as logf:\n",
        "                logf.write(frame_path + '\\n')\n",
        "            processed.add(frame_path)\n",
        "\n",
        "            return True  # Newly processed\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üí• Error processing {frame_path}: {e}\")\n",
        "\n",
        "    time.sleep(0.05)\n",
        "    return False\n",
        "\n",
        "def process_all_images(directory_path, max_folders=100):\n",
        "    folders_processed = 0\n",
        "\n",
        "    for folder_name in sorted(os.listdir(directory_path)):\n",
        "        if folders_processed >= max_folders:\n",
        "            break\n",
        "\n",
        "        folder_path = os.path.join(directory_path, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            jpg_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.jpg')])\n",
        "            newly_processed_in_folder = 0\n",
        "            total_in_folder = len(jpg_files)\n",
        "\n",
        "            for file_name in jpg_files:\n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "                if process_frame(file_path):\n",
        "                    newly_processed_in_folder += 1\n",
        "\n",
        "            print(f\"üìÇ Done with folder {folder_name}: {total_in_folder} total, {newly_processed_in_folder} newly processed\")\n",
        "            folders_processed += 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    directory_path = '/content/drive/MyDrive/FaceForensicsData/frames_for_original_videos'\n",
        "    process_all_images(directory_path, max_folders=100)\n",
        "    print(f\"üéâ Done! Processed up to 100 folders.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWy-We-eSZ69"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv2mt6HeR13t"
      },
      "source": [
        "get the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjgX3HWGSbHp"
      },
      "source": [
        "put them into v1 and v2 inception resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sbdp11TSe_Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load model\n",
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "# Transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "])\n",
        "\n",
        "# Base paths (Limit to only one video from each category)\n",
        "data_sources = {\n",
        "    \"/content/drive/MyDrive/FaceForensicsData/frames_for_original_videos\": 0,   # Real\n",
        "    \"/content/drive/MyDrive/FaceForensicsData/frames_for_FaceSwap\": 1 # Fake\n",
        "}\n",
        "\n",
        "# Hold all embeddings and labels\n",
        "all_embeddings = []\n",
        "all_labels = []\n",
        "\n",
        "# Process only the first video from both real and fake datasets\n",
        "for folder_path, label in data_sources.items():\n",
        "    # Select the first video folder from the directory (only one video folder)\n",
        "    video_folders = sorted(os.listdir(folder_path))[:1]  # Limit to the first folder\n",
        "\n",
        "    for video_folder in tqdm(video_folders, desc=f\"Processing {folder_path}\"):\n",
        "        video_path = os.path.join(folder_path, video_folder)\n",
        "        if not os.path.isdir(video_path):\n",
        "            continue\n",
        "\n",
        "        # Process all frames (100 frames) inside this folder\n",
        "        for img_name in sorted(os.listdir(video_path)):\n",
        "            if not img_name.endswith('.jpg'):\n",
        "                continue\n",
        "\n",
        "            img_path = os.path.join(video_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img_pil = Image.fromarray(img_rgb)\n",
        "            img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embedding = model(img_tensor).cpu().numpy().flatten()  # shape: (512,)\n",
        "\n",
        "            # Append the embedding and label to lists\n",
        "            all_embeddings.append(embedding)\n",
        "            all_labels.append(label)\n",
        "\n",
        "# Convert to arrays\n",
        "X = np.array(all_embeddings)  # shape: (num_images, 512)\n",
        "y = np.array(all_labels)      # shape: (num_images,)\n",
        "\n",
        "# Save embeddings and labels to Google Drive\n",
        "save_path = '/content/drive/MyDrive/embeddings_and_labels/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save embeddings and labels to Google Drive\n",
        "np.save(os.path.join(save_path, \"test_embeddings.npy\"), X)\n",
        "np.save(os.path.join(save_path, \"test_labels.npy\"), y)\n",
        "\n",
        "print(f\"‚úÖ Finished processing. Embeddings shape: {X.shape}, Labels shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf01cwrmSlAH"
      },
      "source": [
        "get the labels and save them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLnUOiTzSjy_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from tqdm import tqdm\n",
        "from itertools import islice  # Import for limiting folder count\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load model\n",
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "# Transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "])\n",
        "\n",
        "# Base paths\n",
        "data_sources = {\n",
        "    \"/content/drive/MyDrive/FaceForensicsData/frames_for_original_videos\": 0,   # Real\n",
        "    \"/content/drive/MyDrive/FaceForensicsData/frames_for_FaceSwap\": 1 # Fake\n",
        "}\n",
        "\n",
        "# Hold all embeddings and labels\n",
        "all_embeddings = []\n",
        "all_labels = []\n",
        "\n",
        "# Process only the first 100 folders from both real and fake datasets\n",
        "for folder_path, label in data_sources.items():\n",
        "    # Use islice to limit the number of folders processed (100 folders from each category)\n",
        "    video_folders = list(islice(sorted(os.listdir(folder_path)), 100))  # Limit to first 100 folders\n",
        "\n",
        "    for video_folder in tqdm(video_folders, desc=f\"Processing {folder_path}\"):\n",
        "        video_path = os.path.join(folder_path, video_folder)\n",
        "        if not os.path.isdir(video_path):\n",
        "            continue\n",
        "\n",
        "        # Process all frames (100 frames) inside each folder\n",
        "        for img_name in sorted(os.listdir(video_path)):\n",
        "            if not img_name.endswith('.jpg'):\n",
        "                continue\n",
        "\n",
        "            img_path = os.path.join(video_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img_pil = Image.fromarray(img_rgb)\n",
        "            img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embedding = model(img_tensor).cpu().numpy().flatten()  # shape: (512,)\n",
        "\n",
        "            # Append the embedding and label to lists\n",
        "            all_embeddings.append(embedding)\n",
        "            all_labels.append(label)\n",
        "\n",
        "# Convert to arrays\n",
        "X = np.array(all_embeddings)  # shape: (num_images, 512)\n",
        "y = np.array(all_labels)      # shape: (num_images,)\n",
        "\n",
        "# Save embeddings and labels to Google Drive\n",
        "save_path = '/content/drive/MyDrive/embeddings_and_labels/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save embeddings and labels to Google Drive\n",
        "np.save(os.path.join(save_path, \"all_embeddings.npy\"), X)\n",
        "np.save(os.path.join(save_path, \"all_labels.npy\"), y)\n",
        "\n",
        "print(f\"‚úÖ Finished processing. Embeddings shape: {X.shape}, Labels shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qzITFfISsHv"
      },
      "outputs": [],
      "source": [
        "for v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FgcUAo9StOj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from tqdm import tqdm\n",
        "from itertools import islice  # Import for limiting folder count\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load model\n",
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "# Transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "])\n",
        "\n",
        "# Base paths\n",
        "data_sources = {\n",
        "    \"/content/drive/MyDrive/FaceForensicsData/frames_for_original_videos\": 0,   # Real\n",
        "    \"/content/drive/MyDrive/FaceForensicsData/frames_for_FaceSwap\": 1 # Fake\n",
        "}\n",
        "\n",
        "# Hold all embeddings and labels\n",
        "all_embeddings = []\n",
        "all_labels = []\n",
        "\n",
        "# Process only the first 100 folders from both real and fake datasets\n",
        "for folder_path, label in data_sources.items():\n",
        "    # Use islice to limit the number of folders processed (100 folders from each category)\n",
        "    video_folders = list(islice(sorted(os.listdir(folder_path)), 100))  # Limit to first 100 folders\n",
        "\n",
        "    for video_folder in tqdm(video_folders, desc=f\"Processing {folder_path}\"):\n",
        "        video_path = os.path.join(folder_path, video_folder)\n",
        "        if not os.path.isdir(video_path):\n",
        "            continue\n",
        "\n",
        "        # Process all frames (100 frames) inside each folder\n",
        "        for img_name in sorted(os.listdir(video_path)):\n",
        "            if not img_name.endswith('.jpg'):\n",
        "                continue\n",
        "\n",
        "            img_path = os.path.join(video_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img_pil = Image.fromarray(img_rgb)\n",
        "            img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embedding = model(img_tensor).cpu().numpy().flatten()  # shape: (512,)\n",
        "\n",
        "            # Append the embedding and label to lists\n",
        "            all_embeddings.append(embedding)\n",
        "            all_labels.append(label)\n",
        "\n",
        "# Convert to arrays\n",
        "X = np.array(all_embeddings)  # shape: (num_images, 512)\n",
        "y = np.array(all_labels)      # shape: (num_images,)\n",
        "\n",
        "# Save embeddings and labels to Google Drive\n",
        "save_path = '/content/drive/MyDrive/embeddings_and_labels/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save embeddings and labels to Google Drive as v2 versions\n",
        "np.save(os.path.join(save_path, \"v2_embeddings.npy\"), X)\n",
        "np.save(os.path.join(save_path, \"v2_labels.npy\"), y)\n",
        "\n",
        "print(f\"‚úÖ Finished processing. Embeddings shape: {X.shape}, Labels shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from models.mem_task2_model import BiLSTMAttentionModel\n",
        "\n",
        "class BiLSTMAttentionModel(nn.Module):\n",
        "    def __init__(self, input_dim=1024, num_classes=2):\n",
        "        super(BiLSTMAttentionModel, self).__init__()\n",
        "\n",
        "        self.feature_enhancer = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # First BiLSTM layer (128 hidden units, bidirectional)\n",
        "        self.bilstm1 = nn.LSTM(256, 128, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Second BiLSTM layer (64 hidden units, bidirectional)\n",
        "        self.bilstm2 = nn.LSTM(128 * 2, 64, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Attention mechanism over final BiLSTM output\n",
        "        self.attention = nn.Linear(64 * 2, 1)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, 1024]\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        x = self.feature_enhancer(x)           # [B, T, 256]\n",
        "        x, _ = self.bilstm1(x)                 # [B, T, 256]\n",
        "        x, _ = self.bilstm2(x)                 # [B, T, 128]\n",
        "\n",
        "        attn_weights = F.softmax(self.attention(x), dim=1)  # [B, T, 1]\n",
        "        context = torch.sum(attn_weights * x, dim=1)        # [B, 128]\n",
        "\n",
        "        out = self.classifier(context)         # [B, 2]\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q18eDD_RplJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from models.mem_task2_model import BiLSTMAttentionModel\n",
        "\n",
        "# ===== Load and preprocess data =====\n",
        "all_features = np.load(\"/content/drive/MyDrive/deepfake_detection/data/combined_embeddings.npy\")\n",
        "all_labels = np.load(\"/content/drive/MyDrive/deepfake_detection/data/combined_labels.npy\")\n",
        "\n",
        "# Trim to multiple of 8\n",
        "total_frames = (len(all_features) // 8) * 8\n",
        "all_features = all_features[:total_frames]\n",
        "all_labels = all_labels[:total_frames]\n",
        "\n",
        "# Reshape and prepare labels\n",
        "all_features = all_features.reshape(-1, 8, 1024)\n",
        "all_labels = all_labels.reshape(-1, 8)\n",
        "majority_labels = np.array([np.bincount(seq).argmax() for seq in all_labels])\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_features, majority_labels, test_size=0.2, stratify=majority_labels, random_state=42)\n",
        "np.save(\"/content/drive/MyDrive/deepfake_detection/data/test_features.npy\", X_test)\n",
        "np.save(\"/content/drive/MyDrive/deepfake_detection/data/test_labels.npy\", y_test)\n",
        "\n",
        "# Prepare training data (frame-level to sequence)\n",
        "train_features = X_train.reshape(-1, 1024)\n",
        "train_labels = np.repeat(y_train, 8)\n",
        "\n",
        "# ===== Dataset class =====\n",
        "class FeatureSequenceDataset(Dataset):\n",
        "    def __init__(self, features, labels, sequence_length=8):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_sequences = len(self.features) // sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.sequence_length\n",
        "        end = start + self.sequence_length\n",
        "        sequence = self.features[start:end]\n",
        "        label_seq = self.labels[start:end]\n",
        "        label = np.bincount(label_seq).argmax()\n",
        "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = FeatureSequenceDataset(train_features, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# ===== Model, Loss, Optimizer =====\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTMAttentionModel(input_dim=1024, num_classes=2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# ===== Training loop =====\n",
        "num_epochs = 10\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for sequences, labels in train_loader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# ===== Save trained model =====\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/deepfake_detection/mem_task2_bilstm_adam.pth\")\n",
        "print(\"‚úÖ Model trained using Adam and saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from models.mem_task2_model import BiLSTMAttentionModel\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ===== Load test data =====\n",
        "features = np.load(\"/content/drive/MyDrive/deepfake_detection/data/test_features.npy\")   # shape: [N, 8, 1024]\n",
        "labels = np.load(\"/content/drive/MyDrive/deepfake_detection/data/test_labels.npy\")       # shape: [N]\n",
        "\n",
        "# ===== Load trained model from Adam training =====\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTMAttentionModel(input_dim=1024, num_classes=2).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/deepfake_detection/mem_task2_bilstm_adam.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Adam-trained model loaded. Starting evaluation on test set...\")\n",
        "\n",
        "# ===== Run inference =====\n",
        "video_preds = []\n",
        "for seq in features:\n",
        "    input_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to(device)  # shape: [1, 8, 1024]\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)                         # shape: [1, 2]\n",
        "        probs = torch.softmax(output, dim=1)                 # softmax over output\n",
        "        pred = torch.argmax(probs, dim=1).item()             # final predicted class\n",
        "        video_preds.append(pred)\n",
        "\n",
        "# ===== Evaluate =====\n",
        "acc = accuracy_score(labels, video_preds)\n",
        "prec = precision_score(labels, video_preds)\n",
        "rec = recall_score(labels, video_preds)\n",
        "f1 = f1_score(labels, video_preds)\n",
        "\n",
        "print(\"\\nüéØ Adam Model Evaluation Results:\")\n",
        "print(f\"Accuracy : {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall   : {rec:.4f}\")\n",
        "print(f\"F1 Score : {f1:.4f}\")\n",
        "\n",
        "# ===== Save results to CSV =====\n",
        "df = pd.DataFrame({\n",
        "    \"Video_Index\": list(range(len(labels))),\n",
        "    \"Predicted_Label\": video_preds,\n",
        "    \"True_Label\": labels.tolist()\n",
        "})\n",
        "df.to_csv(\"/content/drive/MyDrive/deepfake_detection/task3_video_results_adam.csv\", index=False)\n",
        "print(\"üìÅ Adam video-level predictions saved to task3_video_results_adam.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
